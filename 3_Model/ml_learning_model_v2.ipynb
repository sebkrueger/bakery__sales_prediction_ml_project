{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# read datasets into datadframe\n",
    "train_set = pd.read_pickle('../train_val_test_data/train_set.pkl')\n",
    "validation_set = pd.read_pickle('../train_val_test_data/validation_set.pkl')\n",
    "test_set = pd.read_pickle('../train_val_test_data/test_set.pkl')\n",
    "\n",
    "# Remove all NaN rows / TODO: investigate and improve this \n",
    "train_set = train_set.dropna(subset=['Umsatz'])\n",
    "validation_set = validation_set.dropna(subset=['Umsatz'])\n",
    "\n",
    "# check if NaN left\n",
    "#print(test_set.isna().sum())\n",
    "\n",
    "\n",
    "#train_set.head()\n",
    "\n",
    "train_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(data_df):\n",
    "    # Define categorical features\n",
    "    categorical_features = ['Warengruppe', 'Wind_Kategorie']\n",
    "\n",
    "    # Inspect data types and unique values for categorical columns\n",
    "    #print(data_df[categorical_features].dtypes)\n",
    "    #print(\"Unique Values:\\n\",data_df[categorical_features].apply(lambda x: x.unique()))\n",
    "\n",
    "    # Ensure categorical columns are treated as categories\n",
    "    for col in categorical_features:\n",
    "        data_df[col] = data_df[col].astype('category')\n",
    "\n",
    "    # Encode categorical variables using pd.get_dummies\n",
    "    features = pd.get_dummies(data_df[categorical_features], drop_first=True, dtype=int)\n",
    "\n",
    "    # Include any numeric columns that are not categorical\n",
    "    features['rainday'] = data_df['rainday']\n",
    "    features['Salesindex'] = data_df['Salesindex']\n",
    "    features['monthly_mean_temp_diff'] = data_df['monthly_mean_temp_diff']\n",
    "    features['sunny'] = data_df['sunny']\n",
    "    features['KielerWoche'] = data_df['KielerWoche']\n",
    "    features['BinHoly'] = data_df['BinHoly']\n",
    "    features['BinSchhol'] = data_df['BinSchhol']\n",
    "    features['weekend'] = data_df['weekend']\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    # numerical_features = ['rainday', 'Salesindex', 'monthly_mean_temp_diff', 'sunny', 'KielerWoche', 'BinHoly', 'BinSchhol', 'weekend', 'day_of_week', 'month', 'quarter']\n",
    "    # scaler = StandardScaler()\n",
    "    # features[numerical_features] = scaler.fit_transform(features[numerical_features])\n",
    "\n",
    "    # Construct the prepared data set including the dependent variable ('label')\n",
    "    # prepared_data = pd.concat([data_df[['Umsatz']], features], axis=1).dropna()\n",
    "\n",
    "    return features\n",
    "\n",
    "# We need separated features and labels\n",
    "\n",
    "# create the lable first\n",
    "training_features =prepare_features(train_set)\n",
    "validation_features = prepare_features(validation_set)\n",
    "test_features = prepare_features(test_set)\n",
    "\n",
    "\n",
    "# Add Umsatz to training and validation lables\n",
    "training_labels = train_set[['Umsatz']]\n",
    "validation_labels = validation_set[['Umsatz']]\n",
    "# test_data contains no lable for Umsatz \n",
    "\n",
    "#test_labels = test_data[['Umsatz']]\n",
    "\n",
    "# Display the shape of the prepared data set\n",
    "#print(training_features.shape)\n",
    "# Display the first few rows of the prepared data set\n",
    "#print(training_features.head(5))\n",
    "\n",
    "#print(training_labels.shape)\n",
    "#print(training_labels.head(5))\n",
    "\n",
    "#print(train_set.head(5))\n",
    "\n",
    "# Convert DataFrames to numpy arrays\n",
    "training_features_array = training_features.to_numpy()\n",
    "training_labels_array = training_labels.to_numpy().flatten()  # Flatten in case labels are a DataFrame with one column\n",
    "\n",
    "\n",
    "# check for infinite numbers\n",
    "#print(np.isfinite(training_features).all())\n",
    "#print(np.isfinite(training_labels).all())\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(training_features):\n",
    "    X_train, X_val = training_features.iloc[train_index], training_features.iloc[val_index]\n",
    "    y_train, y_val = training_labels.iloc[train_index], training_labels.iloc[val_index]\n",
    "    \n",
    "\n",
    "#Increasing the number of units and adding dropout layers to help prevent overfitting by randomly deactivating neurons during training\n",
    "model = Sequential([\n",
    "    InputLayer(shape=(training_features.shape[1], )),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# Early stopping and Learning Rate Reduction to improve training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# improved code by specifying the batch size\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=32,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"python_model_test.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "training_predictions = model.predict(training_features)\n",
    "validation_predictions = model.predict(validation_features)\n",
    "print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n",
    "print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(data['actual'], label='Actual Values', color='red')\n",
    "    plt.plot(data['prediction'], label='Predicted Values', color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Case Number')\n",
    "    plt.ylabel('Price in 1.000 USD')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Ensure that training_predictions, validation_predictions, training_labels, and validation_labels are numpy arrays\n",
    "training_predictions = np.array(training_predictions).flatten()\n",
    "validation_predictions = np.array(validation_predictions).flatten()\n",
    "training_labels = np.array(training_labels).flatten()\n",
    "validation_labels = np.array(validation_labels).flatten()\n",
    "\n",
    "# Create DataFrames with 1-dimensional arrays\n",
    "data_train = pd.DataFrame({'prediction': training_predictions, 'actual': training_labels})\n",
    "data_validation = pd.DataFrame({'prediction': validation_predictions, 'actual': validation_labels})\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(data_train.head(100), 'Predicted and Actual Values for the Training Data')\n",
    "plot_predictions(data_validation.head(100), 'Predicted and Actual Values for the Validation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions for testdata\n",
    "test_predictions = model.predict(test_features)\n",
    "\n",
    "# flatten and convert from numpy into pandas dataframe\n",
    "test_predictions = np.array(test_predictions).flatten()\n",
    "test_predictions_df = pd.DataFrame({'Umsatz': test_predictions})\n",
    "\n",
    "test_predictions_df.head()\n",
    "\n",
    "test_set.head()\n",
    "\n",
    "# merge the IDs from test date togther with Umsatz predictions\n",
    "# TODO: check if we have any problems because of no common date index on both dataframes\n",
    "submission_set = pd.concat([test_set['id'], test_predictions_df['Umsatz']], axis=1)\n",
    "\n",
    "# Check if the count of dataset is correct for kaggle upload\n",
    "if submission_set.shape[0] == 1830:\n",
    "    print(\"OK : DataFrame has exact 1830 Entries!\")\n",
    "else:\n",
    "    print(f\"ERROR Dataframe has wrong number of {submission_set.shape[0]} Entries!\")\n",
    "\n",
    "# store the submission data\n",
    "submission_set.to_csv('../prediction_data/submission_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
